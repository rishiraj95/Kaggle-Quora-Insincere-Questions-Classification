{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
    "import re\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "bb40886b64534d7a8c0e424d3f2033e984ca9194"
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2b77f6a1c831c98851143feb25c9903cb1154bf2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "sub = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28c5b2320f5ef863df3d0b4e4d9175c59bd61ef0"
   },
   "source": [
    "## Data overview\n",
    "\n",
    "This is a kernel competition, where we can't use external data. As a result we can use only train and test datasets as well as embeddings which were provided by organizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1558909b0a5c120c1d5ddc5be4f5a952fcb4971e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings: ['paragram_300_sl999', 'glove.840B.300d', 'wiki-news-300d-1M', 'GoogleNews-vectors-negative300']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Available embeddings:', os.listdir(\"../input/embeddings/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0c7299c8895405ef00049595ead1ef89649ba71b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdbe2595e31608b72cfbdc8d4bfc75840bfe3a0d"
   },
   "source": [
    "We have a seriuos disbalance - only ~6% of data are positive. No wonder the metric for the competition is f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "afaa845d44d72b9997ce037ab547ab4010701311"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0817c86624cc43ea1df0eba310ba41f4f799f8da"
   },
   "source": [
    "In the dataset we have only texts of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "54a553b7e92a2a0a3d491ccf92b437011b813c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n",
      "Average word length of questions in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7861669f72f36145c25911926a51bc688f51d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n",
      "Max word length of questions in test is 87.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1b1303137eb44cc0de3329921751c48209037562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n",
      "Average character length of questions in test is 70.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b82f366ef8b46b0115e9940c7966849023545733"
   },
   "source": [
    "As we can see on average questions in train and test datasets are similar, but there are quite long questions in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4f1838e686d67670bb6429b8b43619a69803fde8"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "# Clean the text\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "# Clean numbers\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "# Clean speelings\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "9d081b2f0d46faf01a943c309568c27f92462f94"
   },
   "outputs": [],
   "source": [
    "max_features = 120000\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(train['question_text'].values) + list(test['question_text'].values)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "33929a60e1872b73e40424daa1178a3d8fbf8f5a"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\n",
    "test_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4330f6c01064b5fda4ca9661dc4f1cefb439cf75"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
    "plt.yscale('log');\n",
    "plt.title('Distribution of question text length in characters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05c7e2c63afb343b64835432721a42f81dd53626"
   },
   "source": [
    "We can see that most of the questions are 40 words long or shorter. Let's try having sequence length equal to 70 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "15d7e3a2fb5fe5fb38da45ca3d0bd13c82b7eda5"
   },
   "outputs": [],
   "source": [
    "max_len = 72\n",
    "maxlen = 72\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50ade6e2c4dab8ee4add2a9c080f132e012cedc7"
   },
   "source": [
    "### Preparing data for Pytorch\n",
    "\n",
    "Pytorch requires special dataloaders. I'll write a class for it.\n",
    "At first I'll append padded texts to original DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "31839c56ed1d7945decf176c97b49f0205cc40af"
   },
   "outputs": [],
   "source": [
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "69d325ea77439697e53143adef3a0da58e7f31f2"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cca23aa2dc1207a6f08212ad8d87f8182e567e0e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "splits = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=10).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "744ee4a1fbc66cb47aae6a18f829dea284b860c9"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "embedding_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "# all_embs = np.stack(embedding_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std = -0.005838499, 0.48782197\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "cc553dd7ddbd3a81ed1a39084fdb4982b5887971"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore') if len(o)>100)\n",
    "# all_embs = np.stack(embedding_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std = -0.0053247833, 0.49346462\n",
    "embedding_matrix1 = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix1[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "333429eaa0c7c057769518c8f1b0fefbb68f1d1d"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.mean([embedding_matrix, embedding_matrix1], axis=0)\n",
    "del embedding_matrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed3cd4b2f73c7ed5d1896f6b43029c8efdd89d5e"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "23b0dcfa9bff61ef4c5aebd85f9521edd8f6009d"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(1024, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "efa73786bf93892a5c3f46329122c0c0275e4858"
   },
   "outputs": [],
   "source": [
    "m = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a29c7d10faaf54c58cda8a406cb31fb367816f6c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=5)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "        \n",
    "        if validate:\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n",
    "\n",
    "            val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        else:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    \n",
    "    valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "    \n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    print('Validation loss: ', avg_val_loss)\n",
    "\n",
    "    test_preds = np.zeros((len(test_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    scheduler.step()\n",
    "    \n",
    "    return valid_preds, test_preds#, test_preds_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "2252ff85cae8b5096e7f10ccf64ce10299a18782"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\n",
    "test_emb = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "batch_size = 512\n",
    "test_loader = torch.utils.data.DataLoader(test_emb, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "9ec7632d3f2ef30b2e985a4662bc8178b9a49a94"
   },
   "outputs": [],
   "source": [
    "seed=1029\n",
    "\n",
    "# Function to search through different thresholds seperate the choice of 0 or 1 for classification of sincerity\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "68aa1d6303e656ca76184d53c44001836c431939",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=0.1325 \t val_loss=0.1042 \t val_f1=0.6638 best_t=0.25 \t time=303.90s\n",
      "Epoch 2/5 \t loss=0.1149 \t val_loss=0.0991 \t val_f1=0.6793 best_t=0.36 \t time=306.56s\n",
      "Epoch 3/5 \t loss=0.1078 \t val_loss=0.0976 \t val_f1=0.6869 best_t=0.35 \t time=306.61s\n",
      "Epoch 4/5 \t loss=0.1013 \t val_loss=0.0974 \t val_f1=0.6869 best_t=0.38 \t time=306.59s\n",
      "Epoch 5/5 \t loss=0.0938 \t val_loss=0.1024 \t val_f1=0.6806 best_t=0.35 \t time=306.69s\n",
      "Validation loss:  0.10243182599579537\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=0.1302 \t val_loss=0.1045 \t val_f1=0.6624 best_t=0.31 \t time=307.07s\n",
      "Epoch 2/5 \t loss=0.1142 \t val_loss=0.0996 \t val_f1=0.6764 best_t=0.35 \t time=306.48s\n",
      "Epoch 3/5 \t loss=0.1072 \t val_loss=0.0976 \t val_f1=0.6845 best_t=0.33 \t time=306.13s\n",
      "Epoch 4/5 \t loss=0.1005 \t val_loss=0.0974 \t val_f1=0.6850 best_t=0.37 \t time=306.20s\n",
      "Epoch 5/5 \t loss=0.0924 \t val_loss=0.1000 \t val_f1=0.6799 best_t=0.37 \t time=306.39s\n",
      "Validation loss:  0.1000171827566049\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=0.1320 \t val_loss=0.1062 \t val_f1=0.6569 best_t=0.28 \t time=306.08s\n",
      "Epoch 2/5 \t loss=0.1144 \t val_loss=0.1006 \t val_f1=0.6741 best_t=0.33 \t time=306.21s\n",
      "Epoch 3/5 \t loss=0.1077 \t val_loss=0.0989 \t val_f1=0.6818 best_t=0.30 \t time=306.03s\n",
      "Epoch 4/5 \t loss=0.1017 \t val_loss=0.0988 \t val_f1=0.6835 best_t=0.31 \t time=306.00s\n",
      "Epoch 5/5 \t loss=0.0941 \t val_loss=0.1027 \t val_f1=0.6784 best_t=0.29 \t time=305.89s\n",
      "Validation loss:  0.10273536179131532\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(train))\n",
    "test_preds = np.zeros((len(test_emb), len(splits)))\n",
    "n_epochs = 5\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    seed_everything(seed + i)\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    valid_preds_fold, test_preds_fold = train_model(model,\n",
    "                                                                           x_train_fold, \n",
    "                                                                           y_train_fold, \n",
    "                                                                           x_val_fold, \n",
    "                                                                           y_val_fold, validate=True)\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds[:, i] = test_preds_fold\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f5bc2be00ac29b03779c95d334eed0273f56974"
   },
   "source": [
    "## Detect insincere words and mark posts containing them as insincere (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "eabbbb1809b08c4b24c66253a3e09d7e9eebbe9f"
   },
   "outputs": [],
   "source": [
    "# Bad words that appear significantly more frequently in insincere data\n",
    "bad_words = ['Sandler', 'a55', 'anal', 'anus', 'apeshit', 'arse', 'arsehole', 'ass', 'asses', 'asshole', 'assholes', 'asswhole', 'balls', 'ballsack', 'bareback', 'bastard', 'bastards', 'beastiality', 'bestial', 'bestiality', 'bimbo', 'bimbos', 'bitch', 'bitchass', 'bitches', 'bitching', 'bitchy', 'bloody', 'blowjob', 'blowjobs', 'boner', 'boners', 'boob', 'boobies', 'boobs', 'booger', 'booty', 'bosom', 'breasts', 'bullshit', 'bullshits', 'bum', 'busty', 'butt', 'buttfuck', 'butthole', 'buttplug', 'caca', 'cameltoe', 'chink', 'chode', 'circlejerk', 'clit', 'clitoris', 'clits', 'clusterfuck', 'cocain', 'cocaine', 'cock', 'cockhead', 'cocks', 'cocksucker', 'cocksuckers', 'cocksucking', 'commie', 'condom', 'coon', 'coons', 'crap', 'crappy', 'creampie', 'cretin', 'cripple', 'cum', 'cums', 'cumshot', 'cumshots', 'cumslut', 'cunnilingus', 'cunt', 'cunts', 'cus', 'dago', 'dagos', 'damn', 'damned', 'darn', 'deepthroat', 'dick', 'dickhead', 'dickheads', 'dickhole', 'dicks', 'dicksucker', 'dildo', 'dildos', 'dimwit', 'dingle', 'dipshit', 'dirty', 'doggy-style', 'doggystyle', 'domination', 'dong', 'doofus', 'douche', 'douchebag', 'douchebags', 'douchey', 'dumbass', 'dumbasses', 'dumbfuck', 'dykes', 'ejaculate', 'ejaculated', 'ejaculates', 'ejaculating', 'erect', 'erection', 'eunuch', 'fack', 'faggot', 'fags', 'fanny', 'fart', 'fatass', 'fecal', 'fellate', 'fellatio', 'femdom', 'fingering', 'fisted', 'fisting', 'flaps', 'fondle', 'fook', 'fuc', 'fuck', 'fucked', 'fucker', 'fuckers', 'fuckin', 'fucking', 'fucks', 'fuckup', 'fuckwit', 'fuk', 'gae', 'gangbang', 'gangbanged', 'gangbangs', 'ganja', 'gay', 'gays', 'genitals', 'gey', 'goatse', 'god', 'goddam', 'goddamn', 'gonads', 'gooks', 'grope', 'handjob', 'hardcore', 'hell', 'ho', 'hoe', 'homo', 'homoerotic', 'honkey', 'hooker', 'hooters', 'horniest', 'horny', 'hump', 'humped', 'humping', 'inbred', 'incest', 'intercourse', 'jackass', 'jackasses', 'jerk', 'jerked', 'jerkoff', 'jizz', 'junkie', 'kafir', 'kike', 'kill', 'kinky', 'lesbian', 'lesbians', 'looney', 'lust', 'lusting', 'lusty', 'masterbate', 'masturbate', 'masturbating', 'menses', 'midget', 'milf', 'mofo', 'molest', 'moron', 'motherfucker', 'motherfuckers', 'motherfuckin', 'motherfucking', 'murder', 'naked', 'napalm', 'nazi', 'nazism', 'negro', 'nigga', 'niggas', 'nigger', 'niggers', 'nipples', 'nude', 'nudity', 'nympho', 'omg', 'opium', 'orgasms', 'orgies', 'orgy', 'ovums', 'paedophile', 'paki', 'panties', 'panty', 'pedo', 'pedophile', 'pedophilia', 'pee', 'peepee', 'penetrate', 'penile', 'penis', 'perversion', 'phallic', 'pimp', 'pinko', 'piss', 'pissed', 'pisses', 'pissing', 'playboy', 'pms', 'poon', 'poop', 'porn', 'porno', 'pornography', 'pornos', 'potty', 'prick', 'pricks', 'pron', 'prostitute', 'prude', 'psycho', 'pubes', 'pubic', 'puss', 'pussies', 'pussy', 'pussys', 'queef', 'queer', 'queers', 'rape', 'raped', 'raper', 'rapey', 'raping', 'rapist', 'rectum', 'retard', 'retarded', 'rimming', 'rubbish', 'rump', 'sadism', 'scantily', 'scat', 'schizo', 'schlong', 'screw', 'screwed', 'screwing', 'scrotum', 'scum', 'seduce', 'semen', 'sex', 'sexual', 'sexy', 'sh1t', 'shagger', 'shagging', 'shemale', 'shit', 'shite', 'shithead', 'shitheads', 'shithole', 'shithouse', 'shits', 'shitt', 'shitting', 'shitty', 'shiz', 'shota', 'sissy', 'skank', 'slave', 'sleazy', 'slut', 'sluts', 'smartass', 'smartasses', 'smegma', 'snatch', 'snuff', 'sodomy', 'spade', 'sperm', 'stfu', 'stoned', 'strapon', 'strappado', 'strip', 'stupid', 'suck', 'sucked', 'sucking', 'sucks', 'swastika', 'tawdry', 'testicle', 'threesome', 'throating', 'thug', 'titfuck', 'tits', 'titties', 'titty', 'toots', 'topless', 'tramp', 'tranny', 'transsexual', 'trashy', 'trumped', 'turd', 'twat', 'twats', 'twink', 'ugly', 'upskirt', 'vagina', 'vibrator', 'virgin', 'voyeur', 'vulgar', 'vulva', 'wank', 'wanker', 'wedgie', 'weenie', 'weiner', 'weirdo', 'whore', 'whorehouse', 'whores', 'whoring', 'willy', 'womb', 'wtf', 'xx', 'xxx', 'zoophilia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "b6bc759541d0aca3f6c52cc899aaf57eb97b422a"
   },
   "outputs": [],
   "source": [
    "# Import packages for tokenizing sentences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "fd83eeaf0af467c7b2a0c5803087ad7e9a771446"
   },
   "outputs": [],
   "source": [
    "# Function to flag whether or not there is a bad word in a given text\n",
    "# Output 0 for no bad word, 1 for bad word\n",
    "def detect_badwords(text):\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        for bad_word in bad_words:\n",
    "            if bad_word == word:\n",
    "                return 1         \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "0262f848c3e05665ac37fb769aea9e8ac57bc62e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6846d1e52afa40a4b486aa275ef9d205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Register `pandas.progress_apply' with `tqdm`\n",
    "tqdm.pandas()\n",
    "# Apply to the test dataset\n",
    "vec = test['question_text'].progress_apply(detect_badwords)\n",
    "# Add as a column of test dataset\n",
    "test['bad_word'] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "e86e5571f3a6e6e14551a6212e5a45f190304d54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>bad_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>05a6f65d8c4a25ee18f0</td>\n",
       "      <td>how can you kill the laziness in you ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18931</th>\n",
       "      <td>55c6aba78e2ca7a05790</td>\n",
       "      <td>why is it that many conservatives ,  while calling for deportations and quasi - lynch mobs  ( which actually kill people )  ,  are so offended and apparently hurt by words that it seems to make them cry inside ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37717</th>\n",
       "      <td>aa3d6a2b66b6c95d401e</td>\n",
       "      <td>how many references are associated with sex in the quran ,  hadith ,  muhammad ,  and revelations ?  is submitted more about the expectation of a 7th century arab man and what a woman is required to do as slave or wife ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37267</th>\n",
       "      <td>a820158b23dc37952ad9</td>\n",
       "      <td>why do i pee after a bowel movement ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9377</th>\n",
       "      <td>2b36d0463ef108cd4590</td>\n",
       "      <td>how many times sex in a day ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        qid   ...    bad_word\n",
       "1220   05a6f65d8c4a25ee18f0   ...           1\n",
       "18931  55c6aba78e2ca7a05790   ...           1\n",
       "37717  aa3d6a2b66b6c95d401e   ...           1\n",
       "37267  a820158b23dc37952ad9   ...           1\n",
       "9377   2b36d0463ef108cd4590   ...           1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sample of test data containing bad words\n",
    "test[test['bad_word']==1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "840d1275856ea3dbad93ea465b445e4d53445c42"
   },
   "outputs": [],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "# Best predictions from the model\n",
    "pred_model = [int(i) for i in (test_preds.mean(1) > search_result['threshold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e6b02413d9e5f8039f48a3c474de8e261102aeb5"
   },
   "outputs": [],
   "source": [
    "# Vector that flags bad words\n",
    "pred_bw = test['bad_word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "9abce028a40a5094e96282909111e451168fabf2"
   },
   "outputs": [],
   "source": [
    "# Find the union of 1s between the two lists\n",
    "def vec_or(v1,v2):\n",
    "    return [max(v1[i],v2[i]) for i in range(len(v1))]\n",
    "\n",
    "pred_comb = vec_or(pred_model,pred_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "2c04e298f5c96a9561450874a0238f408984c39e"
   },
   "outputs": [],
   "source": [
    "# Organise submission data and export\n",
    "search_result = threshold_search(y_train, train_preds)\n",
    "sub['prediction'] = pred_comb\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
