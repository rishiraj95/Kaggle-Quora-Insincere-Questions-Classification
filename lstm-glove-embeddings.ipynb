{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21f85dfd5b9bf3d8b27ba29149d52253e5d64049"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb2cb07e3e6542c997417196d43308a57448e830"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78578eab64a477d0a5ad6b1c917ae154868a44df"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "#Â split up training set into a smaller trainng set and a validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92ffbf2ef35d2dc5863ee27c61eadd1869ca5440"
   },
   "outputs": [],
   "source": [
    "# embdedding setup\n",
    "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index = {}\n",
    "f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cc8b0bcd285225ce651d024f506615d4656b9f7"
   },
   "outputs": [],
   "source": [
    "# Convert text (string) to an array of embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = text[:-1].split()[:30] # taking only up to 30 words in text\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text] # if embedding for word does not exist, use empyt_emb\n",
    "    embeds+= [empyt_emb] * (30 - len(embeds)) # add empty embeddings to make all embeddings sets lenght 30\n",
    "    return np.array(embeds)\n",
    "\n",
    "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][:3000])])\n",
    "val_y = np.array(val_df[\"target\"][:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c950448e0717eaebb920d93cfdc6b4561e21853"
   },
   "outputs": [],
   "source": [
    "# Data providers - use multiple batches for training\n",
    "batch_size = 128\n",
    "\n",
    "# function to generate batches\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data. (frac- fraction of set to sample from)\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5e08503d31bc42e7fe470157f604d10d466fe28"
   },
   "outputs": [],
   "source": [
    "df=train_df.sample(1000)\n",
    "batch_gen(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "706c0224b6112e5a8f00ad25f35e90fbb9519a5f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "798c303ec834fb530a60a1e590cfbd9a86f93fde"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c1b43751cede1e1ef26f1123420e33f63e09f66"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def prec(y_true,y_pred):\n",
    "    true_positives=K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n",
    "    pred_positives=K.sum(K.round(K.clip(y_pred,0,1)))\n",
    "    precision=true_positives/(pred_positives+K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def rec(y_true,y_pred):\n",
    "    true_positives=K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n",
    "    possible_positives=K.sum(K.round(K.clip(y_true,0,1)))\n",
    "    recall=true_positives/(possible_positives+K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true,y_pred):\n",
    "    true_positives=K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n",
    "    possible_positives=K.sum(K.round(K.clip(y_true,0,1)))\n",
    "    pred_positives=K.sum(K.round(K.clip(y_pred,0,1)))\n",
    "    precision=true_positives/(pred_positives+K.epsilon())\n",
    "    recall=true_positives/(possible_positives+K.epsilon())\n",
    "    return (2*precision*recall)/(precision+recall+K.epsilon())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ad418c95b31ab5691d49b72c7c9622ef9ea42cf"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n",
    "                        input_shape=(30, 300)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score,prec,rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f0b703ded691293450beb4ddf2d903d0e08c737"
   },
   "outputs": [],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=20,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6cdda0e301be0b84e826e29f0f3a85c41aa6da9"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b58cd95254f41e5002a17de0c3feab54a5fc3c67"
   },
   "outputs": [],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e6ed54def110c881f401c6f8a844752baedcfbd"
   },
   "outputs": [],
   "source": [
    "# put data in integer form - predictions for test data\n",
    "y_te = (np.array(all_preds) > 0.5).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "728ee6d366719ed2bf7d0f59d87f74475bfc0273"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
